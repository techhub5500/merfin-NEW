/**
 * Junior Agent - Simple User Response Agent with Conversational Memory
 *
 * NOVO SISTEMA DE MEM√ìRIA:
 * - Mem√≥ria conversacional simplificada baseada em ciclos
 * - √öltimos 4 ciclos mantidos integralmente  
 * - Ciclos anteriores comprimidos progressivamente
 * - Limite m√°ximo de 3.000 tokens de contexto
 */

const BaseAgent = require('../../shared/base-agent');
const memoryIntegration = require('../../../core/memory/memory-integration-new');
const OpenAI = require('openai');

// Inicializa√ß√£o lazy do cliente OpenAI
let openai = null;
function getOpenAI() {
  if (!openai) {
    openai = new OpenAI({
      apiKey: process.env.OPENAI_API_KEY,
    });
  }
  return openai;
}

class JuniorAgent extends BaseAgent {
  constructor() {
    super('JuniorAgent');
    
    this.model = 'gpt-4.1-mini';
    this.max_output_tokens = 1500;
  }

  /**
   * M√©todo principal de execu√ß√£o do agente
   * @param {Object} request - Requisi√ß√£o do usu√°rio
   * @returns {Promise<Object>} Resposta do agente
   */
  async execute(request) {
    const { parameters } = request;
    return await this.processChatMessage(parameters);
  }

  /**
   * Processa uma mensagem de chat completa
   * @param {Object} params - Par√¢metros da mensagem
   * @returns {Promise<Object>} Resposta processada
   */
  async processChatMessage(params) {
    const { message, sessionId, history, userId, chatId } = params;

    try {
      // Valida√ß√£o b√°sica da mensagem
      if (!message || typeof message !== 'string' || message.trim().length === 0) {
        throw new Error('Mensagem inv√°lida ou vazia');
      }

      // Inicializa sess√£o se necess√°rio
      if (sessionId && userId) {
        try {
          memoryIntegration.initializeSession(sessionId, userId, {
            startedAt: new Date().toISOString(),
            chatId: chatId
          });
        } catch (error) {
          console.log('[JuniorAgent] Sess√£o j√° existe:', error.message);
        }
      }

      // Constr√≥i contexto de mem√≥ria conversacional
      let memoryContext = null;
      if (sessionId && chatId && userId) {
        try {
          console.log('[JuniorAgent] üîç Carregando contexto de mem√≥ria...');
          memoryContext = await memoryIntegration.buildAgentContext(sessionId, chatId, userId);
          console.log('[JuniorAgent] ‚úÖ Contexto carregado:', memoryContext.stats);
        } catch (error) {
          console.warn('[JuniorAgent] Erro ao carregar mem√≥ria:', error.message);
        }
      }

      // Monta prompt do sistema
      const systemPrompt = this._buildSystemPrompt();
      
      // Monta contexto de mem√≥ria
      let contextualInput = '';
      if (memoryContext?.conversationalContext) {
        contextualInput = memoryContext.conversationalContext + '\n\n';
      }
      
      // Adiciona mensagem atual
      contextualInput += `U: ${message}\nA:`;

      // Log do prompt
      console.log('[JuniorAgent] üìù Prompt constru√≠do:', {
        systemLength: systemPrompt.length,
        contextLength: contextualInput.length,
        estimatedTokens: Math.ceil((systemPrompt.length + contextualInput.length) / 4)
      });

      // Log do prompt completo enviado √† IA
      console.log('[AI_PROMPT] ü§ñ PROMPT COMPLETO ENVIADO PARA IA:', {
        model: this.model,
        system_prompt: systemPrompt,
        user_context: contextualInput,
        max_tokens: this.max_output_tokens,
        temperature: 0.7,
        sessionId,
        chatId,
        userId
      });

      // Gera resposta usando OpenAI Chat Completions
      const response = await getOpenAI().chat.completions.create({
        model: this.model,
        messages: [
          { role: 'system', content: systemPrompt },
          { role: 'user', content: contextualInput }
        ],
        max_tokens: this.max_output_tokens,
        temperature: 0.7
      });
      
      // Log de consumo de tokens
      if (response?.usage) {
        const usage = response.usage;
        console.log('[JuniorAgent] üí∞ Tokens consumidos:', {
          prompt: usage.prompt_tokens,
          completion: usage.completion_tokens,
          total: usage.total_tokens
        });
      }
      
      const responseText = response.choices[0]?.message?.content?.trim();

      if (!responseText) {
        console.error('[JuniorAgent] ‚ùå Resposta vazia da API');
        throw new Error('Resposta vazia da API');
      }

      const finalResponse = responseText;

      // Processa mem√≥rias em background (n√£o bloqueante)
      if (sessionId && chatId && userId) {
        memoryIntegration.processInteractionMemories({
          sessionId,
          chatId,
          userId,
          userMessage: message,
          aiResponse: finalResponse
        }).catch(error => {
          console.error('[JuniorAgent] Erro no processamento de mem√≥ria:', error);
        });
      }

      return {
        response: finalResponse,
        sessionId: sessionId,
        timestamp: new Date().toISOString()
      };

    } catch (error) {
      console.error('[JuniorAgent] Erro no processamento:', error);
      return {
        response: 'Desculpe, houve um erro ao processar sua mensagem. Tente novamente.',
        sessionId: sessionId,
        timestamp: new Date().toISOString()
      };
    }
  }

  /**
   * Constr√≥i prompt do sistema
   * @returns {string} - System prompt
   */
  _buildSystemPrompt() {
    return `Voc√™ √© um assistente financeiro pessoal amig√°vel e direto.

## Regras de comunica√ß√£o:
1. Seja conciso e acolhedor - evite longas listas logo de in√≠cio
2. Perguntas diretas merecem respostas diretas
3. Use tom amig√°vel, primeira pessoa, tutear o usu√°rio
4. M√°ximo 3-4 linhas para respostas iniciais; expanda s√≥ se pedido
5. Se o usu√°rio j√° compartilhou informa√ß√µes no hist√≥rico, USE essas informa√ß√µes
6. N√£o repita informa√ß√µes que o usu√°rio j√° sabe

## Formato de resposta:
- Responda em portugu√™s brasileiro natural
- Use emojis com modera√ß√£o (1-2 por mensagem no m√°ximo)
- Seja objetivo e √∫til`;
  }
}

module.exports = JuniorAgent;